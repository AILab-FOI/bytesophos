# app/services/evaluation.py

"""Module for evaluating question-answering pipeline outputs using Ragas metrics.

This module loads a ground truth dataset, queries a codebase, collects answers and
contexts, and evaluates them using Ragas metrics such as answer relevancy,
context precision, and faithfulness. It produces a JSON report summarizing the
evaluation results.
"""

import os
import json
import re
import asyncio
import concurrent.futures
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple, Iterable
from dataclasses import dataclass, asdict

from app.services.pipeline import query_codebase
from app.schemas.query import QueryRequest
from app import db

from haystack_integrations.components.evaluators.ragas import RagasEvaluator
from ragas.llms import HaystackLLMWrapper
from ragas.metrics import AnswerRelevancy, ContextPrecision, Faithfulness
from haystack.components.generators import OpenAIGenerator
from haystack.utils import Secret

DATASET_PATH = Path(os.getenv("EVAL_DATASET", "backend/pipeline/app/eval/lagger_test_data.json"))
RESULTS_PATH = Path(os.getenv("EVAL_RESULTS", "eval/ragas_report.json"))
REPO_ID = "84f6ff236fb448a1a308120200864dfc"

PROCESS_WORKERS = int(os.getenv("EVAL_PROC_WORKERS", "2"))
BATCH_SIZE = int(os.getenv("EVAL_BATCH_SIZE", "1"))
TIMEOUT_S = int(os.getenv("EVAL_TIMEOUT", "240"))
EVAL_LIMIT = int(os.getenv("EVAL_LIMIT", "200"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY must be set for Ragas/OpenAI evaluation")

THINK_TAG_RE = re.compile(r"(?is)<\s*think\s*>.*?<\s*/\s*think\s*>")                # assisted with chatgpt5
THINK_BRACKET_RE = re.compile(r"(?is)\[\s*think\s*\].*?\[\s*/\s*think\s*\]")        # assisted with chatgpt5
THOUGHT_PREFIX_RE = re.compile(r"(?im)^\s*(thought|thinking|reasoning)\s*:\s.*?$")  # assisted with chatgpt5

# assisted with chatgpt5
def strip_think(text: Optional[str]) -> str:
    """Remove hidden reasoning or 'think' tags from text.

    Args:
        text: Input text to clean.

    Returns:
        Cleaned text with reasoning tags and prefixes removed.
    """
    if not isinstance(text, str):
        return ""
    t = THINK_TAG_RE.sub("", text)
    t = THINK_BRACKET_RE.sub("", t)
    t = THOUGHT_PREFIX_RE.sub("", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    t = re.sub(r"[ \t]+", " ", t)
    return t.strip()

@dataclass
class EvalRow:
    """Represents a single evaluation item from the dataset.

    Attributes:
        question: The evaluation question.
        ground_truth: The ground truth answer, if available.
    """
    question: str
    ground_truth: Optional[str] = None

@dataclass
class ExampleResult:
    """Represents the evaluation result for a single example.

    Attributes:
        question: The question being evaluated.
        answer: The answer generated by the pipeline.
        contexts: The supporting context documents.
        ground_truth: The ground truth answer, if available.
        metrics: Computed metric scores.
    """
    question: str
    answer: str
    contexts: List[str]
    ground_truth: Optional[str]
    metrics: Optional[Dict[str, float]] = None

def load_dataset(path: Path, limit: int = 0) -> List[EvalRow]:
    """Load a dataset of evaluation rows from a JSON file.

    Args:
        path: Path to the dataset JSON file.
        limit: Maximum number of rows to load (0 for all).

    Returns:
        A list of EvalRow instances.

    Raises:
        FileNotFoundError: If the dataset file does not exist.
        ValueError: If the dataset is not a non-empty list.
    """
    if not path.exists():
        raise FileNotFoundError(f"Dataset not found: {path}")
    raw = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(raw, list) or not raw:
        raise ValueError("Dataset must be a non-empty JSON array.")

    rows: List[EvalRow] = []
    for item in raw[: (limit or len(raw))]:
        q = item.get("question")
        if isinstance(q, str) and q.strip():
            gt = (item.get("reference") or item.get("answer") or item.get("ground_truth") or None)
            gt = strip_think(gt.strip()) if isinstance(gt, str) else None
            rows.append(EvalRow(question=q.strip(), ground_truth=gt))
        else:
            print(f"[dataset] Skipping item with non-string question: {type(q)} -> {q!r}")
    return rows

def _worker_process(
    rows_payload: List[Tuple[str, Optional[str]]],
    repo_id: str,
    timeout_s: int
) -> List[Dict[str, Any]]:
    """Process a batch of evaluation rows in a worker subprocess.

    Args:
        rows_payload: A list of (question, ground_truth) tuples.
        repo_id: The repository identifier for querying codebase.
        timeout_s: Timeout for each query in seconds.

    Returns:
        A list of result dictionaries containing question, answer, contexts, and ground_truth.
    """

    async def _run_one(question: str, gt: Optional[str]) -> Dict[str, Any]:
        req = QueryRequest(question=question, repoId=repo_id)
        try:
            answer, contexts = await asyncio.wait_for(query_codebase(req), timeout=timeout_s)
        except asyncio.TimeoutError:
            answer, contexts = ("", [])

        answer = strip_think(answer)

        ctxs: List[str] = []
        for c in (contexts or []):
            text = (c.get("content") if isinstance(c, dict) else c) or ""
            text = text.strip()
            if text:
                ctxs.append(text[:2000])

        return {
            "question": question,
            "answer": answer,
            "contexts": ctxs,
            "ground_truth": gt,
        }

    async def _run_batch() -> List[Dict[str, Any]]:
        await db.init_pool()
        try:
            out: List[Dict[str, Any]] = []
            for q, gt in rows_payload:
                out.append(await _run_one(q, gt))
            return out
        finally:
            await db.close_pool()

    return asyncio.run(_run_batch())

def collect_examples_via_subprocess(rows: List[EvalRow]) -> List[ExampleResult]:
    """Collect example results using multiple subprocess workers.

    Args:
        rows: A list of evaluation rows.

    Returns:
        A list of ExampleResult instances with pipeline answers and contexts.
    """
    groups: List[List[EvalRow]] = []
    i = 0
    while i < len(rows):
        groups.append(rows[i:i + BATCH_SIZE])
        i += BATCH_SIZE

    results: List[ExampleResult] = []
    print(f"[runner] Spawning up to {PROCESS_WORKERS} worker processes, batches={len(groups)}, batch_size={BATCH_SIZE}")

    with concurrent.futures.ProcessPoolExecutor(max_workers=PROCESS_WORKERS) as pool:
        futures = []
        for batch in groups:
            payload = [(r.question, r.ground_truth) for r in batch]
            futures.append(pool.submit(_worker_process, payload, REPO_ID, TIMEOUT_S))

        for fut in concurrent.futures.as_completed(futures):
            try:
                batch_out = fut.result()
            except Exception as e:
                print(f"[runner] Batch future failed: {e}")
                continue

            for item in batch_out:
                results.append(
                    ExampleResult(
                        question=item["question"],
                        answer=item["answer"],
                        contexts=item["contexts"],
                        ground_truth=item["ground_truth"],
                    )
                )
            print(f"[runner] Collected {len(results)} results ...")

    return results

def build_ragas_evaluator() -> RagasEvaluator:
    """Construct a RagasEvaluator instance for metric computation.

    Returns:
        A configured RagasEvaluator with relevant metrics and LLM wrapper.
    """
    llm = OpenAIGenerator(api_key=Secret.from_token(OPENAI_API_KEY))
    wrapper = HaystackLLMWrapper(llm)
    return RagasEvaluator(
        ragas_metrics=[AnswerRelevancy(), ContextPrecision(), Faithfulness()],
        evaluator_llm=wrapper,
    )

def _to_float(x: Any) -> Optional[float]:
    """Convert an object to a float if possible.

    Args:
        x: The input object.

    Returns:
        The float value if conversion succeeds, otherwise None.
    """
    try:
        return float(x)
    except Exception:
        try:
            return float(getattr(x, "value", None))
        except Exception:
            return None

def _flatten_items(d: Any) -> Iterable[Tuple[str, Any]]:
    """Recursively flatten nested dictionaries or objects into key-value pairs.

    Args:
        d: The input object or dictionary.

    Yields:
        Tuples of (key, value) pairs from nested structures.
    """
    if isinstance(d, dict):
        for k, v in d.items():
            yield k, v
            for k2, v2 in _flatten_items(v):
                yield k2, v2
    to_dict = getattr(d, "to_dict", None)
    if callable(to_dict):
        try:
            dd = to_dict()
            if isinstance(dd, dict):
                for k, v in _flatten_items(dd):
                    yield k, v
        except Exception:
            pass

def _scores_from_result_obj(result_obj: Any) -> Dict[str, float]:
    """Extract metric scores from a Ragas evaluation result object.

    Args:
        result_obj: The Ragas evaluation result.

    Returns:
        A dictionary of metric names and float scores.
    """
    scores_attr = getattr(result_obj, "scores", None)
    if isinstance(scores_attr, list) and scores_attr:
        merged: Dict[str, float] = {}
        for row in scores_attr:
            if isinstance(row, dict):
                for k, v in row.items():
                    fv = _to_float(v)
                    if isinstance(fv, float):
                        merged[k] = fv
        if merged:
            return merged
    if isinstance(scores_attr, dict):
        out = {k: _to_float(v) for k, v in scores_attr.items()}
        return {k: v for k, v in out.items() if isinstance(v, float)}

    to_dict = getattr(result_obj, "to_dict", None)
    if callable(to_dict):
        try:
            d = to_dict()
            if isinstance(d, dict):
                sc = d.get("scores")
                if isinstance(sc, list) and sc:
                    merged: Dict[str, float] = {}
                    for row in sc:
                        if isinstance(row, dict):
                            for k, v in row.items():
                                fv = _to_float(v)
                                if isinstance(fv, float):
                                    merged[k] = fv
                    if merged:
                        return merged
                if isinstance(sc, dict):
                    out = {k: _to_float(v) for k, v in sc.items()}
                    return {k: v for k, v in out.items() if isinstance(v, float)}
                out = {k: _to_float(v) for k, v in d.items()}
                filtered = {k: v for k, v in out.items() if isinstance(v, float)}
                if filtered:
                    return filtered
        except Exception:
            pass

    if isinstance(result_obj, dict):
        out = {k: _to_float(v) for k, v in result_obj.items()}
        return {k: v for k, v in out.items() if isinstance(v, float)}

    known = {"answer_relevancy", "context_precision", "faithfulness"}
    found: Dict[str, float] = {}
    for k, v in _flatten_items(result_obj):
        if k in known:
            fv = _to_float(v)
            if isinstance(fv, float):
                found[k] = fv
    return found

def evaluate_examples(evaluator: RagasEvaluator, examples: List[ExampleResult]) -> Dict[str, float]:
    """Evaluate a list of examples using the Ragas evaluator.

    Args:
        evaluator: A configured RagasEvaluator instance.
        examples: A list of ExampleResult objects to evaluate.

    Returns:
        A dictionary summarizing average metric scores across examples.
    """
    buckets: Dict[str, List[float]] = {}

    for i, ex in enumerate(examples):
        kwargs: Dict[str, Any] = {
            "query": ex.question,
            "documents": ex.contexts or [],
            "response": strip_think(ex.answer or ""),
        }
        if isinstance(ex.ground_truth, str) and ex.ground_truth.strip():
            kwargs["reference"] = strip_think(ex.ground_truth.strip())

        try:
            out = evaluator.run(**kwargs)
        except Exception as e:
            print(f"[ragas] Sample {i} failed: {e}")
            ex.metrics = None
            continue

        result_obj = (out or {}).get("result")
        scores = _scores_from_result_obj(result_obj)
        if not scores:
            scores = _scores_from_result_obj(out or {})
        if not scores:
            print(f"[ragas] Sample {i} returned empty metrics payload")
            ex.metrics = None
            continue

        ex.metrics = scores
        for k, v in scores.items():
            if isinstance(v, (int, float)):
                buckets.setdefault(k, []).append(float(v))

    def _avg(xs: List[float]) -> Optional[float]:
        return round(sum(xs) / len(xs), 4) if xs else None

    return {
        "answer_relevancy": _avg(buckets.get("answer_relevancy", [])),
        "context_precision": _avg(buckets.get("context_precision", [])),
        "faithfulness": _avg(buckets.get("faithfulness", [])),
    }

def main():
    """Main entry point for running the evaluation process.

    Loads the dataset, queries the pipeline, evaluates results,
    and writes a summary report to disk.
    """
    rows = load_dataset(DATASET_PATH, EVAL_LIMIT)
    print(f"[ragas] Loaded {len(rows)} ground-truth questions from {DATASET_PATH}")

    examples = collect_examples_via_subprocess(rows)
    print(f"[ragas] Collected {len(examples)} pipeline outputs.")

    evaluator = build_ragas_evaluator()
    summary = evaluate_examples(evaluator, examples)

    report = {
        "repo_id": REPO_ID,
        "n": len(examples),
        "metrics": summary,
        "examples": [asdict(e) for e in examples],
    }
    RESULTS_PATH.parent.mkdir(parents=True, exist_ok=True)
    RESULTS_PATH.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding="utf-8")

    print("\n=== Ragas Summary (Integrated Evaluator) ===")
    print(f"answer_relevancy: {summary.get('answer_relevancy')}")
    print(f"context_precision: {summary.get('context_precision')}")
    print(f"faithfulness: {summary.get('faithfulness')}")
    print(f"\nSaved report → {RESULTS_PATH}")

if __name__ == "__main__":
    main()
